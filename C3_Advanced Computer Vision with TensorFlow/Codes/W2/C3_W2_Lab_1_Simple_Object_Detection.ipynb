{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmANPR2jhCR6"
      },
      "source": [
        "# Simple Object Detection in Tensorflow\n",
        "\n",
        "This lab will walk you through how to use object detection models available in [Tensorflow Hub](https://www.tensorflow.org/hub). In the following sections, you will:\n",
        "\n",
        "* explore the Tensorflow Hub for object detection models\n",
        "* load the models in your workspace\n",
        "* preprocess an image for inference\n",
        "* run inference on the models and inspect the output\n",
        "\n",
        "Let's get started!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DkMLuGDhCR6"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEoRKdmByrb0"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from PIL import Image\n",
        "from PIL import ImageOps\n",
        "import tempfile\n",
        "from six.moves.urllib.request import urlopen\n",
        "from six import BytesIO"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nb8MBgTOhCR6"
      },
      "source": [
        "### Download the model from Tensorflow Hub\n",
        "\n",
        "Tensorflow Hub is a repository of trained machine learning models which you can reuse in your own projects.\n",
        "- You can see the domains covered [here](https://tfhub.dev/) and its subcategories.\n",
        "- For this lab, you will want to look at the [image object detection subcategory](https://tfhub.dev/s?module-type=image-object-detection).\n",
        "- You can select a model to see more information about it and copy the URL so you can download it to your workspace.\n",
        "- We selected a [inception resnet version 2](https://tfhub.dev/google/faster_rcnn/openimages_v4/inception_resnet_v2/1)\n",
        "- You can also modify this following cell to choose the other model that we selected, [ssd mobilenet version 2](https://tfhub.dev/tensorflow/ssd_mobilenet_v2/2)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9pCzz4uy20U"
      },
      "source": [
        "# you can switch the commented lines here to pick the other model\n",
        "\n",
        "# inception resnet version 2\n",
        "module_handle = \"https://tfhub.dev/google/faster_rcnn/openimages_v4/inception_resnet_v2/1\"\n",
        "\n",
        "# You can choose ssd mobilenet version 2 instead and compare the results\n",
        "#module_handle = \"https://tfhub.dev/google/openimages_v4/ssd/mobilenet_v2/1\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3trj5FbhCR6"
      },
      "source": [
        "#### Load the model\n",
        "\n",
        "Next, you'll load the model specified by the `module_handle`.\n",
        "- This will take a few minutes to load the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WHkGDHfhCR6"
      },
      "source": [
        "model = hub.load(module_handle)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ey0FpHGhCR6"
      },
      "source": [
        "#### Choose the default signature\n",
        "\n",
        "Some models in the Tensorflow hub can be used for different tasks. So each model's documentation should show what *signature* to use when running the model.\n",
        "- If you want to see if a model has more than one signature then you can do something like `print(hub.load(module_handle).signatures.keys())`. In your case, the models you will be using only have the `default` signature so you don't have to worry about other types."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1BU7AGthCR6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b55a3284-cfac-47ea-874b-f1e6382bc18c"
      },
      "source": [
        "# take a look at the available signatures for this particular model\n",
        "model.signatures.keys()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KeysView(_SignatureMap({'default': <ConcreteFunction pruned(images) at 0x7DD9CC54F1C0>}))"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfc9ax9hhCR6"
      },
      "source": [
        "Please choose the 'default' signature for your object detector.\n",
        "- For object detection models, its 'default' signature will accept a batch of image tensors and output a dictionary describing the objects detected, which is what you'll want here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzwR5zE_hCR7"
      },
      "source": [
        "detector = model.signatures['default']"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wvb-3r3thCR7"
      },
      "source": [
        "### download_and_resize_image\n",
        "\n",
        "This function downloads an image specified by a given \"url\", pre-processes it, and then saves it to disk."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ucsxak_qhCR7"
      },
      "source": [
        "def download_and_resize_image(url, new_width=256, new_height=256):\n",
        "    '''\n",
        "    Fetches an image online, resizes it and saves it locally.\n",
        "\n",
        "    Args:\n",
        "        url (string) -- link to the image\n",
        "        new_width (int) -- size in pixels used for resizing the width of the image\n",
        "        new_height (int) -- size in pixels used for resizing the length of the image\n",
        "\n",
        "    Returns:\n",
        "        (string) -- path to the saved image\n",
        "    '''\n",
        "\n",
        "\n",
        "    # create a temporary file ending with \".jpg\"\n",
        "    _, filename = tempfile.mkstemp(suffix=\".jpg\")\n",
        "\n",
        "    # opens the given URL\n",
        "    response = urlopen(url)\n",
        "\n",
        "    # reads the image fetched from the URL\n",
        "    image_data = response.read()\n",
        "\n",
        "    # puts the image data in memory buffer\n",
        "    image_data = BytesIO(image_data)\n",
        "\n",
        "    # opens the image\n",
        "    pil_image = Image.open(image_data)\n",
        "\n",
        "    # resizes the image. will crop if aspect ratio is different.\n",
        "    pil_image = ImageOps.fit(pil_image, (new_width, new_height), Image.ANTIALIAS)\n",
        "\n",
        "    # converts to the RGB colorspace\n",
        "    pil_image_rgb = pil_image.convert(\"RGB\")\n",
        "\n",
        "    # saves the image to the temporary file created earlier\n",
        "    pil_image_rgb.save(filename, format=\"JPEG\", quality=90)\n",
        "\n",
        "    print(\"Image downloaded to %s.\" % filename)\n",
        "\n",
        "    return filename"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7qodEJHhCR7"
      },
      "source": [
        "### Download and preprocess an image\n",
        "\n",
        "Now, using `download_and_resize_image` you can get a sample image online and save it locally.\n",
        "- We've provided a URL for you, but feel free to choose another image to run through the object detector.\n",
        "- You can use the original width and height of the image but feel free to modify it and see what results you get."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHTDalVrhCR7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a304e7eb-bc9e-4876-df7f-3ccae91c1845"
      },
      "source": [
        "# You can choose a different URL that points to an image of your choice\n",
        "image_url = \"https://upload.wikimedia.org/wikipedia/commons/f/fb/20130807_dublin014.JPG\"\n",
        "\n",
        "# download the image and use the original height and width\n",
        "downloaded_image_path = download_and_resize_image(image_url, 3872, 2592)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-4e9bff65542d>:31: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n",
            "  pil_image = ImageOps.fit(pil_image, (new_width, new_height), Image.ANTIALIAS)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image downloaded to /tmp/tmpcn74_3hr.jpg.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVNXUKMIhCR7"
      },
      "source": [
        "### run_detector\n",
        "\n",
        "This function will take in the object detection model `detector` and the path to a sample image, then use this model to detect objects and display its predicted class categories and detection boxes.\n",
        "- run_detector uses `load_image` to convert the image into a tensor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkkiQzKlhCR7"
      },
      "source": [
        "def load_img(path):\n",
        "    '''\n",
        "    Loads a JPEG image and converts it to a tensor.\n",
        "\n",
        "    Args:\n",
        "        path (string) -- path to a locally saved JPEG image\n",
        "\n",
        "    Returns:\n",
        "        (tensor) -- an image tensor\n",
        "    '''\n",
        "\n",
        "    # read the file\n",
        "    img = tf.io.read_file(path)\n",
        "\n",
        "    # convert to a tensor\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "\n",
        "    return img\n",
        "\n",
        "\n",
        "def run_detector(detector, path):\n",
        "    '''\n",
        "    Runs inference on a local file using an object detection model.\n",
        "\n",
        "    Args:\n",
        "        detector (model) -- an object detection model loaded from TF Hub\n",
        "        path (string) -- path to an image saved locally\n",
        "    '''\n",
        "\n",
        "    # load an image tensor from a local file path\n",
        "    img = load_img(path)\n",
        "\n",
        "    # add a batch dimension in front of the tensor\n",
        "    converted_img  = tf.image.convert_image_dtype(img, tf.float32)[tf.newaxis, ...]\n",
        "\n",
        "    # run inference using the model\n",
        "    result = detector(converted_img)\n",
        "\n",
        "    # save the results in a dictionary\n",
        "    result = {key:value.numpy() for key,value in result.items()}\n",
        "\n",
        "    # print results\n",
        "    print(\"Found %d objects.\" % len(result[\"detection_scores\"]))\n",
        "\n",
        "    print(result[\"detection_scores\"])\n",
        "    print(result[\"detection_class_entities\"])\n",
        "    print(result[\"detection_boxes\"])\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSEeJSkxhCR7"
      },
      "source": [
        "### Run inference on the image\n",
        "\n",
        "You can run your detector by calling the `run_detector` function. This will print the number of objects found followed by three lists:\n",
        "\n",
        "* The detection scores of each object found (i.e. how confident the model is),\n",
        "* The classes of each object found,\n",
        "* The bounding boxes of each object\n",
        "\n",
        "You will see how to overlay this information on the original image in the next sections and in this week's assignment!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csanHvDIz4_t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55763673-06a2-441e-bada-bc604c462b18"
      },
      "source": [
        "# runs the object detection model and prints information about the objects found\n",
        "run_detector(detector, downloaded_image_path)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 100 objects.\n",
            "[0.6544875  0.61146575 0.6042277  0.5926301  0.59218854 0.5804906\n",
            " 0.55140543 0.49466825 0.47515732 0.4734231  0.43995935 0.4148525\n",
            " 0.40629575 0.39828596 0.3976529  0.37621245 0.372795   0.365749\n",
            " 0.3526071  0.33274707 0.30428615 0.27276596 0.2686444  0.25776598\n",
            " 0.25290832 0.24611554 0.23403922 0.20343061 0.18229072 0.18045811\n",
            " 0.17571175 0.1643497  0.15850599 0.15665148 0.15470897 0.15452732\n",
            " 0.14924932 0.13340658 0.12948053 0.12649752 0.12044303 0.11767512\n",
            " 0.11355797 0.11114918 0.1110016  0.10914893 0.10604026 0.08940533\n",
            " 0.08598413 0.08280193 0.08104295 0.07806055 0.07759735 0.0762865\n",
            " 0.07546813 0.07444097 0.07427172 0.07205172 0.07177514 0.07102289\n",
            " 0.07032681 0.06809764 0.06304497 0.06285997 0.06271035 0.06224036\n",
            " 0.05881918 0.05815071 0.05795657 0.05787636 0.05462402 0.05274354\n",
            " 0.05133754 0.04826405 0.0470848  0.04682906 0.04495562 0.04405171\n",
            " 0.04360496 0.04113379 0.04109997 0.03968291 0.03935013 0.03912617\n",
            " 0.0387967  0.03878459 0.03739692 0.03606886 0.03367107 0.03366901\n",
            " 0.03260141 0.03253572 0.03201497 0.02983092 0.02877995 0.02867511\n",
            " 0.02804236 0.02783229 0.02734371 0.02668286]\n",
            "[b'Person' b'Person' b'Person' b'Person' b'Footwear' b'Person' b'Building'\n",
            " b'Bicycle' b'Building' b'Window' b'Person' b'Bicycle' b'Wheel'\n",
            " b'Building' b'Building' b'Building' b'Person' b'Wheel' b'Window'\n",
            " b'Window' b'Building' b'Person' b'Van' b'Person' b'Bicycle wheel'\n",
            " b'Person' b'Window' b'Window' b'Building' b'Window' b'Window' b'Man'\n",
            " b'Person' b'Woman' b'Person' b'Clothing' b'Bicycle wheel' b'Window'\n",
            " b'Person' b'Window' b'Land vehicle' b'Land vehicle' b'Clothing' b'Window'\n",
            " b'Bicycle' b'Land vehicle' b'House' b'House' b'Man' b'Window' b'Clothing'\n",
            " b'Window' b'Footwear' b'Person' b'Man' b'Man' b'House' b'Building'\n",
            " b'Person' b'Clothing' b'Window' b'Person' b'Man' b'Person' b'Furniture'\n",
            " b'Jeans' b'Person' b'Person' b'Person' b'Land vehicle' b'Window' b'House'\n",
            " b'Woman' b'Man' b'Window' b'Person' b'Person' b'Clothing' b'Man' b'Man'\n",
            " b'Window' b'Car' b'Person' b'Man' b'Chair' b'Car' b'House' b'Window'\n",
            " b'Tire' b'Clothing' b'Window' b'Clothing' b'Land vehicle' b'Window'\n",
            " b'Window' b'Man' b'Van' b'Bus' b'Clothing' b'Car']\n",
            "[[5.1279438e-01 5.2927101e-01 6.0166228e-01 5.5209458e-01]\n",
            " [5.1974612e-01 6.0150719e-01 6.4612430e-01 6.3468295e-01]\n",
            " [5.0574589e-01 5.0044078e-01 6.0134912e-01 5.2308977e-01]\n",
            " [4.8630875e-01 4.1276222e-01 6.7855006e-01 4.5990553e-01]\n",
            " [8.1519085e-01 9.5611840e-01 8.4270161e-01 9.8714471e-01]\n",
            " [4.9546623e-01 9.2353421e-01 8.3563501e-01 9.9905682e-01]\n",
            " [1.1098684e-02 1.1912074e-02 7.3975062e-01 4.2490709e-01]\n",
            " [5.7782596e-01 3.6645320e-01 7.1280563e-01 4.8333824e-01]\n",
            " [7.7493519e-02 4.1305384e-01 5.7945871e-01 5.6030929e-01]\n",
            " [0.0000000e+00 1.1929257e-01 2.2389720e-01 1.8394907e-01]\n",
            " [5.1406950e-01 7.4809796e-01 5.9196234e-01 7.6656908e-01]\n",
            " [5.7077807e-01 3.6182043e-01 7.0732844e-01 4.2966729e-01]\n",
            " [6.3209414e-01 3.5986993e-01 7.0384169e-01 4.1181549e-01]\n",
            " [1.5908178e-02 6.8496186e-01 5.5938947e-01 8.1114709e-01]\n",
            " [0.0000000e+00 7.9710925e-01 6.7373586e-01 1.0000000e+00]\n",
            " [0.0000000e+00 2.1702683e-01 6.5097284e-01 4.3200096e-01]\n",
            " [5.0037271e-01 3.7700450e-01 6.3335079e-01 4.1451436e-01]\n",
            " [6.4034009e-01 4.4502345e-01 7.0303476e-01 4.8345757e-01]\n",
            " [1.9440397e-03 0.0000000e+00 1.3933195e-01 2.6288416e-02]\n",
            " [2.5517973e-03 9.6662551e-01 1.5375265e-01 1.0000000e+00]\n",
            " [1.4156342e-03 1.4104844e-03 7.6484835e-01 2.6935202e-01]\n",
            " [5.0490117e-01 3.6078489e-01 6.3766325e-01 3.8548002e-01]\n",
            " [4.8338354e-01 6.1948419e-01 5.6265807e-01 6.6157198e-01]\n",
            " [4.9820140e-01 3.6461431e-01 6.6115755e-01 4.0489662e-01]\n",
            " [6.3122940e-01 3.6032289e-01 7.0414698e-01 4.1149932e-01]\n",
            " [5.2180678e-01 5.7769489e-01 5.8761311e-01 6.0071772e-01]\n",
            " [2.1960379e-01 3.4873888e-01 3.3825567e-01 3.7706760e-01]\n",
            " [1.2482677e-01 2.5092393e-01 2.7991474e-01 2.8162587e-01]\n",
            " [2.5731844e-01 5.6749368e-01 5.3091025e-01 6.8787652e-01]\n",
            " [4.2175334e-02 8.7476522e-01 2.5286338e-01 9.1304612e-01]\n",
            " [1.5640162e-01 4.4336551e-01 2.2223385e-01 4.7578454e-01]\n",
            " [5.0199431e-01 9.2146748e-01 8.3636183e-01 1.0000000e+00]\n",
            " [5.2367359e-01 5.7034701e-01 5.8450615e-01 5.9160709e-01]\n",
            " [5.1916909e-01 5.9996617e-01 6.4633030e-01 6.3409472e-01]\n",
            " [5.1315480e-01 6.7922848e-01 5.5098128e-01 6.9254816e-01]\n",
            " [5.2434462e-01 9.2494547e-01 8.1052840e-01 9.9797946e-01]\n",
            " [6.3806343e-01 4.4279736e-01 7.0172900e-01 4.8413199e-01]\n",
            " [3.4105524e-02 3.5565764e-01 1.6230482e-01 3.7490878e-01]\n",
            " [4.8809013e-01 4.5336699e-01 6.2225741e-01 4.7966489e-01]\n",
            " [9.6649170e-04 3.0770731e-01 1.0651590e-01 3.3207035e-01]\n",
            " [4.8296997e-01 6.1979175e-01 5.6477904e-01 6.6065258e-01]\n",
            " [5.8239132e-01 3.6492330e-01 7.1389157e-01 4.8468530e-01]\n",
            " [5.2379000e-01 7.4929291e-01 5.8547038e-01 7.6531160e-01]\n",
            " [3.5146433e-01 9.7486877e-01 5.5304354e-01 9.9888706e-01]\n",
            " [6.0907698e-01 4.2683360e-01 7.0519632e-01 4.8710746e-01]\n",
            " [5.6925476e-01 3.5978299e-01 7.0856631e-01 4.2843917e-01]\n",
            " [0.0000000e+00 8.1118721e-01 6.9358259e-01 9.9325359e-01]\n",
            " [1.0429739e-02 2.2947056e-02 7.2731256e-01 4.2228740e-01]\n",
            " [4.8463222e-01 4.1069773e-01 6.9474292e-01 4.6313992e-01]\n",
            " [8.1154369e-02 3.8477585e-01 2.0795214e-01 4.1175538e-01]\n",
            " [5.3856760e-01 6.0358506e-01 6.3474095e-01 6.3447660e-01]\n",
            " [0.0000000e+00 1.2407594e-02 1.4029641e-01 2.4734123e-02]\n",
            " [6.2977976e-01 6.1488312e-01 6.4490795e-01 6.2533468e-01]\n",
            " [5.0284290e-01 3.8242072e-01 5.9601706e-01 4.1271874e-01]\n",
            " [5.1468122e-01 7.4787110e-01 5.9194797e-01 7.6678246e-01]\n",
            " [5.0643313e-01 5.0040275e-01 6.0071695e-01 5.2331972e-01]\n",
            " [0.0000000e+00 2.1112843e-01 6.5082580e-01 4.3438423e-01]\n",
            " [0.0000000e+00 7.0631987e-01 6.1716044e-01 8.6593878e-01]\n",
            " [4.8929802e-01 4.5427495e-01 5.7262003e-01 4.7639754e-01]\n",
            " [5.0920725e-01 4.1626489e-01 6.6901666e-01 4.5957711e-01]\n",
            " [4.6780016e-03 8.0310702e-01 1.5958212e-01 8.4036517e-01]\n",
            " [5.2617568e-01 5.6837577e-01 5.7943642e-01 5.8280307e-01]\n",
            " [5.0284749e-01 3.7398598e-01 6.4712596e-01 4.1297254e-01]\n",
            " [4.8591763e-01 4.4443727e-01 6.2468988e-01 4.7351983e-01]\n",
            " [5.7416856e-01 2.6725125e-01 6.5776157e-01 3.2031399e-01]\n",
            " [6.7198229e-01 9.4031781e-01 8.2117724e-01 9.8921394e-01]\n",
            " [5.2410495e-01 5.6155592e-01 5.7834727e-01 5.8050251e-01]\n",
            " [5.1759005e-01 7.5722110e-01 5.8831400e-01 7.7154601e-01]\n",
            " [5.2332860e-01 5.5781364e-01 5.7902908e-01 5.7355344e-01]\n",
            " [6.1236012e-01 4.2740160e-01 7.0609635e-01 4.8830032e-01]\n",
            " [0.0000000e+00 2.4423710e-01 6.0888749e-02 2.9377383e-01]\n",
            " [1.5484416e-02 1.9419534e-03 7.4516326e-01 2.5933659e-01]\n",
            " [4.9326652e-01 9.2395955e-01 8.3691329e-01 9.9770677e-01]\n",
            " [5.0529301e-01 3.6016637e-01 6.4336216e-01 3.9143836e-01]\n",
            " [8.4341830e-03 2.4212143e-01 4.9744956e-02 2.8314561e-01]\n",
            " [5.2210933e-01 5.3608817e-01 5.9767473e-01 5.5313325e-01]\n",
            " [5.1312578e-01 5.2381003e-01 6.0054022e-01 5.4296499e-01]\n",
            " [5.1831567e-01 5.0345343e-01 5.9754539e-01 5.2275294e-01]\n",
            " [5.2045572e-01 6.0093164e-01 6.4599115e-01 6.3436395e-01]\n",
            " [5.1316828e-01 6.7925376e-01 5.5048609e-01 6.9244295e-01]\n",
            " [4.2972332e-01 8.2874370e-01 5.9004873e-01 8.6437541e-01]\n",
            " [5.2659309e-01 6.2719083e-01 5.6328988e-01 6.5378511e-01]\n",
            " [5.0478113e-01 3.8941076e-01 6.1523169e-01 4.1995156e-01]\n",
            " [5.0132483e-01 3.6423644e-01 6.5975285e-01 4.0372020e-01]\n",
            " [5.7311028e-01 2.6673263e-01 6.6622359e-01 3.1864992e-01]\n",
            " [5.1510262e-01 6.2409157e-01 5.6383234e-01 6.5803188e-01]\n",
            " [8.3203331e-02 4.0756786e-01 5.8434391e-01 5.5831081e-01]\n",
            " [2.8820190e-01 4.6259380e-04 4.1427985e-01 3.6707666e-02]\n",
            " [6.2713259e-01 3.6099511e-01 7.0596063e-01 4.0978032e-01]\n",
            " [4.9715933e-01 4.5521101e-01 5.8427125e-01 4.7787201e-01]\n",
            " [1.1719383e-02 3.0807251e-01 9.7320117e-02 3.2507545e-01]\n",
            " [5.1589411e-01 3.8009059e-01 5.9697264e-01 4.1176715e-01]\n",
            " [5.1242822e-01 6.2364906e-01 5.6243664e-01 6.5768230e-01]\n",
            " [4.0077364e-01 8.8497424e-01 5.8165663e-01 9.3913031e-01]\n",
            " [0.0000000e+00 9.9475821e-03 1.3625400e-01 3.1597447e-02]\n",
            " [5.1390558e-01 5.2950239e-01 6.0205591e-01 5.5237615e-01]\n",
            " [5.1069075e-01 6.2403941e-01 5.6341022e-01 6.5818000e-01]\n",
            " [4.8037976e-01 6.2032783e-01 5.6528413e-01 6.6012347e-01]\n",
            " [5.3840744e-01 9.2802435e-01 7.1361744e-01 9.9945277e-01]\n",
            " [4.8633769e-01 6.2024736e-01 5.6352866e-01 6.6021776e-01]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WNtL3yOXsO3J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}